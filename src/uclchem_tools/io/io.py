from typing import Union
import pandas as pd
import uclchem
from .rates import get_rates_of_change, rates_to_dfs

import os

import warnings
from tables import NaturalNameWarning
import h5py
import numpy as np
from tqdm import tqdm
import pathlib
import logging
import glob

from pathlib import Path

if __name__ == "__main__":
    raise NotImplementedError("A wrapper for this function still needs to be written.")


def df_to_h5py(fh, key, df):
    fh.create_dataset(
        f"/{key}",
        data=df.to_numpy(),
    )
    fh.create_dataset(f"/{key}_header", data=df.columns.values)


def h5py_to_df(fh: Union[str, h5py.File], dataset_key: str) -> pd.DataFrame:
    """Read a dataset that was generated by the GridConverter.

    Args:
        fh (str, h5py.File): Either a h5py file handle, otherwise it will be treated as a h5py file handle.
        dataset_key (str): The key of the dataset to load.

    Returns:
        pd.DataFrame: A dataframe of the dataset you tried to load.
    """
    with h5py.File(fh) if isinstance(fh, str) else fh as _fh:
        if isinstance(dataset_key, list):
            dataset_key = "/".join(dataset_key)
        data = _fh[dataset_key]
        dataset_header_key = dataset_key + "_header"
        if dataset_header_key in _fh:
            header = _fh[dataset_header_key]
            return pd.DataFrame(
                data, columns=[col.decode("UTF-8") for col in header[:]]
            )
        else:
            return pd.DataFrame(data)


def full_output_csv_to_hdf(
    csv_path: str,
    hdf_path: str,
    datakey: str = "",
    get_rates: bool = False,
    derivatives_path: str = None,
    assume_identical_networks: bool = False,
    storage_backend: str = "h5py",
):
    """Convert the full output of UCLCHEM into a HDF datastore.

    Args:
        csv_path (str): The path of the full output csv
        hdf_path (str): The path of the target hdf store
        datakey (str): the key for the particular dataframe. Defaults to "".
        get_rates (bool, optional): Whether to obtain the rates, dramatically reduces performance. Defaults to False.
    """
    abundances_header = None
    species_lookup = None
    if assume_identical_networks is False:
        raise NotImplementedError(
            "Writing and reading to different networks is not yet implemented."
        )
    if storage_backend == "h5py":
        with h5py.File(hdf_path, "a") as fh:
            df = uclchem.analysis.read_output_file(csv_path)
            # cast down to float32 since we lost accurary in custom ascii anyway.
            df = df.astype("float32")
            if not abundances_header:
                # Set the abundances header on first write of a grid.
                abundances_header = df.columns.values
            if (abundances_header == df.columns.values).all():
                df_to_h5py(fh, f"{datakey}/abundances", df)
            else:
                raise RuntimeError(
                    "I found different abundances columns from the first entry, stopping."
                )
            if derivatives_path:
                derivatives = pd.read_csv(derivatives_path, index_col=0)
                df_to_h5py(fh, f"{datakey}/derivatives", derivatives)
            # Add reactions and species from current UCLCHEM install
            reactions = uclchem.utils.get_reaction_table()
            species = uclchem.utils.get_species_table()
            if assume_identical_networks:
                # TODO: refactor with species lookup table to get all integers instead of mixed data types.
                # SPECIE 0 is NAN
                if "index_species_lookup" in fh:
                    isl = fh["index_species_lookup"][:]
                    species_lookup = {k.decode("UTF-8"): int(v) for k, v in isl}
                else:
                    from uclchem.makerates.reaction import reaction_types

                    species_lookup = {
                        spec: i
                        for i, spec in enumerate(
                            ["NAN"] + list(species["NAME"]) + reaction_types
                        )
                    }
                    # Finally write the species to index lookup to the disk as an recarray:
                    # We need legacy "S" support to write to h5py
                    species_lookup_table = np.array(
                        [[k, v] for k, v in species_lookup.items()], dtype="S"
                    )
                    # species_lookup_table = np.stack((names_lookup, indices_lookup))

                    fh.create_dataset(
                        "/index_species_lookup", data=species_lookup_table
                    )
                if "reactions" not in fh:
                    # For all the headers of the reactants/products, replace them with integers.
                    for reaction_header in reactions:
                        if reaction_header.startswith(
                            "Reactant"
                        ) or reaction_header.startswith("Product"):
                            reactions[
                                f"{reaction_header[:4].lower()}_index_{reaction_header[-1]}"
                            ] = (
                                reactions[reaction_header]
                                .apply(lambda x: species_lookup[str(x).upper()])
                                .astype("int32")
                            )
                    # Identical for the names of the species in the index:
                    species["name_index"] = (
                        species["NAME"]
                        .apply(lambda x: species_lookup[x.upper()])
                        .astype("int32")
                    )
                    # Drop the expensive text columns
                    reactions = reactions.drop(
                        [
                            header
                            for header in reactions
                            if header.startswith("Reactant")
                            or header.startswith("Product")
                        ],
                        axis=1,
                    )
                    species = species.drop(["NAME"], axis=1)
                    # Sort the indices, so they move to the front:
                    reactions = reactions[
                        sorted(reactions.columns, key=lambda x: "index" not in x)
                    ]
                    species = species[
                        sorted(species.columns, key=lambda x: "index" not in x)
                    ]

                    df_to_h5py(fh, "/reactions", reactions)
                    df_to_h5py(fh, "/species", species)

            else:
                raise NotImplementedError(
                    "Not yet implemented for individual networks."
                )
        # POSTPROCESS UCLCHEM obtain the rates
        if get_rates:
            reactions = uclchem.utils.get_reaction_table()
            species = uclchem.utils.get_species_table()
            rates_dict = get_rates_of_change(df, species, reactions)
            # Lazy fix to parse both old and new versions species format:
            names = None
            for possible_name_key in ["Name", "NAME", "name"]:
                if possible_name_key in species:
                    names = list(species[possible_name_key])
            if names is None:
                raise RuntimeError(
                    "Could not find the species names in the species table, stopping."
                )
            for specie in names:
                df_rates, df_production, df_destruction = rates_to_dfs(
                    rates_dict, specie
                )
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", category=NaturalNameWarning)
                    with pd.HDFStore(hdf_path) as store:
                        store.put(
                            f"{datakey}/rates/total_rates/{specie}",
                            df_rates,
                            index=False,
                        )
                        store.put(
                            f"{datakey}/rates/production/{specie}",
                            df_production,
                            index=False,
                        )
                        store.put(
                            f"{datakey}/rates/destruction/{specie}",
                            df_destruction,
                            index=False,
                        )


class GridConverter:
    """Convert a list of models run on a grid to a HDF dataset."""

    def __init__(
        self,
        hdf_path: Path,
        model_df: Union[Path, pd.DataFrame],
        abundances_dir: Path = None,
        derivatives_dir: Path = None,
        get_rates: bool = False,
    ):
        """
        Initializes an instance of the IO class.

        Args:
            hdf_path (pathlib.Path): The path to the HDF5 file to store the grid in.
            model_df (Union[Path, pd.DataFrame]): The path to the CSV file containing the model data, or a pandas DataFrame containing the model data.
            abundances_dir (Path, optional): The path to the directory containing the abundance files
                (in case the data was moved compared to the time the model_df was generated). Defaults to None.
            derivatives_dir (Path, optional): The path to the directory containing the derivative files. Defaults to None.
            get_rates (bool, optional): Whether to obtain all rates directly from UCLCHEM. Defaults to False.
        """
        if pathlib.Path(hdf_path).exists():
            raise RuntimeError("The store already exists, stoppping")
        # Make sure the directories are Path objects:
        abundances_dir = Path(abundances_dir) if abundances_dir else None
        derivatives_dir = Path(derivatives_dir) if derivatives_dir else None

        if get_rates:
            logging.warning(
                "Obtaining all rates is very expensive, this will take a while."
            )
        if not isinstance(model_df, pd.DataFrame):
            if not isinstance(model_df, Path):
                model_df = Path(model_df)
            if model_df.suffix == ".csv":
                model_df = pd.read_csv(model_df, index_col=0)
            elif model_df.suffix == ".h5":
                model_df = pd.read_hdf(model_df, "model_df")
            else:
                raise (
                    RuntimeError(
                        "Unknown file format, please provide a csv or hdf file."
                    )
                )
        # See if the files are present in the original location:
        if abundances_dir:
            old_abundance_dir, model_df["storage_path"] = self.process_outputFile(
                model_df["outputFile"]
            )
            model_df["abundances_path"] = [
                str(abundances_dir / file_name)
                for file_name in model_df["storage_path"]
            ]
            model_df["storage_id"] = [
                Path(file_name).stem for file_name in model_df["storage_path"]
            ]
        else:
            model_df["abundances_path"] = model_df["outputFile"]
        if derivatives_dir:
            assert (
                abundances_dir
            ), "You can only run derivatives dir after running abundance_dir."
            # Use the abundances_path to make sure we don't have to do any matching.
            model_df["derivatives_path"] = [
                str(derivatives_dir / file_name)
                for file_name in model_df["abundances_path"]
            ]

        # Save model_df with the model dataframe as a pandas object (inefficient, but we only store it once.)
        model_df["storage_id"] = self.get_storage_id(model_df)
        # Write the model dataframe to the store:
        model_df.to_hdf(hdf_path, "model_df")

        for idx, row in tqdm(enumerate(model_df.iterrows()), total=len(model_df)):
            row = row[1]  # throw away the pandas index
            full_output_csv_to_hdf(
                row["abundances_path"],
                hdf_path,
                row["storage_id"],
                get_rates=get_rates,
                assume_identical_networks=True,
                derivatives_path=row["derivatives_path"] if derivatives_dir else None,
            )

    def process_outputFile(self, output_files, strict_check=False):
        """Function that obtains the common directory and individual filenames of the output files.

        Args:
            output_files (list[Union[str, Path]]): The model dataframe

        Returns:
            common_directory (str): The common directory of the output files

        Example: a directory files following outputFiles:
            - /home/user/some/very/long/and/completely/unnecessary/path/grid_0.csv
            - /home/user/some/very/long/and/completely/unnecessary/path/grid_1.csv
            - /home/user/some/very/long/and/completely/unnecessary/path/grid_2.csv
        Will return:
            (/home/user/some/very/long/and/completely/unnecessary/path/,
            ["grid_0.csv", "grid_1.csv", "grid_2.csv"])
        """
        common_directory = Path(os.path.commonpath((output_files[0], output_files[1])))
        if strict_check:
            for path in output_files:
                if common_directory != Path(path).parent:
                    raise RuntimeError(
                        "The output files are not all in the same directory!"
                    )
        filenames = [Path(output_file).name for output_file in output_files]
        return common_directory, filenames

    def get_storage_id(self, model_df):
        if "storage_id" in model_df:
            return model_df["storage_id"]
        else:
            logging.warning(
                f"Cannot find `storage_id`, enumerating the {len(model_df)} models by grid_0, grid_1, ..."
            )
            return [f"grid_{i}" for i in list(model_df.index)]


class DataLoaderCSV:
    """Loads CSV data from several full output files generated by UCLCHEM"""

    def __init__(
        self,
        csv_directory: str,
        match_statement: str = "*Full.dat",
        get_rates: bool = False,
    ):
        """Dataloader that can be used to load a whole grid of files.

        Args:
            csv_directory (str): _description_
            match_statement (str, optional): _description_. Defaults to "*Full.dat".
            get_rates (bool, optional): _description_. Defaults to False.

        Raises:
            RuntimeError: _description_
            RuntimeError: _description_
        """
        self.get_rates = get_rates
        if self.get_rates:
            logging.warning(
                "get_rates is true, ensure the version that created ran the model (identical version& network) is used\n If not, you might get unexpected behaviour. Currently there are NO checks."
            )
        csv_files = [
            pathlib.Path(p) for p in glob.glob(csv_directory + match_statement)
        ]
        if len(csv_files) == 0:
            raise RuntimeError(
                f"Found zero files, is the path {csv_directory} correct?"
            )
        print(csv_files)
        self.model_df = pd.DataFrame()
        self.model_df["FullOutput"] = csv_files
        self.model_df["storage_id"] = [p.stem for p in self.model_df["FullOutput"]]
        self.species = self.get_species()["NAME"].to_list()
        self.reactions = self.get_reactions()
        self.csv_store = {}
        self.rates_store = {}
        for row in self.model_df.iterrows():
            row = row[1]
            row_id = row["storage_id"]
            if row_id in self.csv_store:
                raise RuntimeError(
                    "Found duplicate entries in the csv directory, make sure all names are unique"
                )
            fulloutput = uclchem.analysis.read_output_file(row["FullOutput"])
            self.csv_store[row_id] = fulloutput
            if self.get_rates:
                rates_dict = get_rates_of_change(
                    fulloutput, self.species, self.reactions
                )
                self.rates_store[row_id] = {
                    "total_rates": {},
                    "production": {},
                    "destruction": {},
                }
                for specie in self.species:
                    total_rates, production, destruction = rates_to_dfs(
                        rates_dict, specie
                    )
                    self.rates_store[row_id]["total_rates"][specie] = total_rates
                    self.rates_store[row_id]["production"][specie] = production
                    self.rates_store[row_id]["destruction"][specie] = destruction
            else:
                self.rates_store[row_id] = {}

    def keys(self):
        return list(self.csv_store.keys())

    def get_species(self):
        try:
            return uclchem.utils.get_species_table()
        except (NameError, AttributeError) as exc:
            raise exc("Cannot find UCLCHEM, so cannot obtain the species table")

    def get_reactions(self):
        try:
            return uclchem.utils.get_reaction_table()
        except (NameError, AttributeError) as exc:
            raise exc("Cannot find UCLCHEM, so cannot obtain the species table")

    def __getitem__(self, key) -> dict:
        return {
            **{
                "abundances": self.csv_store[key],
                "reactions": self.reactions,
                "species": self.species,
            },
            **self.rates_store[key],
        }


class DataLoaderHDF:
    """Loads results as written to a common hdf datastore (see GridConverter for the format)"""

    def __init__(self, h5path, h5mode="r"):
        try:
            self.models_df = pd.read_hdf(h5path, "model_df")
            self.datasets = self.models_df["storage_id"].to_list()
        except KeyError:
            with h5py.File(h5path, mode=h5mode) as fh:
                self.datasets = list(fh.keys())
                print(
                    "No model DataFrame, obtained these keys instead (filter at your own discretion):",
                    self.datasets,
                )
        self.h5path = h5path
        self.h5mode = h5mode
        self.get_rates = "rates" in self.datasets[0]
        self._lookup_index_to_species = self.get_lookup_index_to_species()
        self.species_table = self._load_species_table()
        self.reaction_table = self._load_species_table()
        self.species = list(self.species_table["NAME"])
        self.reactions = None

    def get_h5_filehandle(self) -> h5py.File:
        return h5py.File(self.h5path, self.h5mode)

    def get_lookup_index_to_species(self):
        with self.get_h5_filehandle() as fh:
            lookup = fh["index_species_lookup"][:]
        return {
            b: a.decode("UTF-8") for a, b in zip(lookup[:, 0], lookup[:, 1].astype(int))
        }

    def _load_species_table(self):
        with self.get_h5_filehandle() as fh:
            species_table = h5py_to_df(fh, "species")
        species_table["NAME"] = species_table["name_index"].apply(
            lambda r: self._lookup_index_to_species[r]
        )
        return species_table

    def _load_reactions_table(self):
        with self.get_h5_filehandle() as fh:
            reactions_table = h5py_to_df(fh, "reactions")
        for reaction in [r for r in reactions_table if "_index_" in r]:
            reactions_table[reaction.replace("_index_", " ").upper()] = reactions_table[
                reaction
            ].apply(lambda r: self._lookup_index_to_species[r])
        return reactions_table

    def get_species(self):
        return self.species_table["NAME"]

    def get_reactions(self):
        return self.reactions_table

    def keys(self):
        return self.get_datasets_keys()

    def get_datasets_keys(self):
        return self.datasets

    def __getitem__(self, key) -> dict:
        with self.get_h5_filehandle() as fh:
            if self.get_rates:
                temp_dict = {
                    "total_rates": {
                        spec: pd.read_hdf(fh, f"{key}/rates/total_rates/{spec}")
                        for spec in self.species
                    },
                    "production": {
                        spec: pd.read_hdf(fh, f"{key}/rates/production/{spec}")
                        for spec in self.species
                    },
                    "destruction": {
                        spec: pd.read_hdf(fh, f"{key}/rates/destruction/{spec}")
                        for spec in self.species
                    },
                }
            else:
                temp_dict = {
                    "total_rates": None,
                    "production": None,
                    "destruction": None,
                }
            return {
                "abundances": h5py_to_df(fh, key + "/abundances"),
                "reactions": self.reactions,
                "species": self.species,
                **temp_dict,
            }
